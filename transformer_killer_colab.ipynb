{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Transformer Killer Core - Google Colab\n",
    "\n",
    "This notebook runs the unified Transformer Killer benchmarks on Google Colab with GPU support.\n",
    "\n",
    "**Controllers available:**\n",
    "- `transformer` - Standard Transformer decoder (baseline)\n",
    "- `mamba` - Mamba backbone (Mamba2 CUDA if installed, GRU fallback)\n",
    "- `mamba_dualmem` - Mamba + DualTierMiras parametric memory\n",
    "- `ot_agent` - OT Memory Agent (Mamba + DualTierMiras + optional LTM)\n",
    "\n",
    "**Benchmarks:**\n",
    "- Synthetic: copy_memory, assoc_recall\n",
    "- Language Model: character-level LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your zip file\\nfrom google.colab import files\\nprint(\\\"Upload unified_transformer_mamba_core.zip:\\\")\\nuploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip\\n!unzip -o unified_transformer_mamba_core.zip -d /content\\n%cd /content/unified_transformer_mamba_core\\n!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run unified setup script (installs PyTorch, dependencies, and optionally Mamba CUDA)\\n# This handles everything automatically!\\n!python setup_colab.py --install-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install Mamba2 from LOCAL SOURCE (takes ~5 min to compile)\\n# This gives better performance than the PyPI version\\n# Uncomment to enable:\\n\\n# !python setup_colab.py --install-mamba-source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (Optional) Install Mamba2 CUDA Kernels\n",
    "\n",
    "This enables real Mamba2 SSM layers instead of GRU fallback. Skip if you want faster setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install real Mamba2 (takes ~5 min to compile)\n",
    "# Uncomment to enable:\n",
    "\n",
    "# %cd /content/unified_transformer_mamba_core/external/mamba_ssm\n",
    "# !pip install -e . --quiet\n",
    "# %cd /content/unified_transformer_mamba_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sanity Check\n",
    "\n",
    "Verify all components work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m transformer_killer_core.unified_bench --sanity_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synthetic Benchmarks\n",
    "\n",
    "### 4.1 Copy Memory Task\n",
    "\n",
    "Tests ability to copy a sequence after a delay period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer baseline\n",
    "!python -m transformer_killer_core.unified_bench \\\n",
    "    --mode synthetic --task copy_memory \\\n",
    "    --controller transformer \\\n",
    "    --seq_len 100 --delay 40 \\\n",
    "    --epochs 20 --batch_size 64 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mamba baseline\n",
    "!python -m transformer_killer_core.unified_bench \\\n",
    "    --mode synthetic --task copy_memory \\\n",
    "    --controller mamba \\\n",
    "    --seq_len 100 --delay 40 \\\n",
    "    --epochs 20 --batch_size 64 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mamba + DualTierMiras (the \"killer\")\n",
    "!python -m transformer_killer_core.unified_bench \\\n",
    "    --mode synthetic --task copy_memory \\\n",
    "    --controller mamba_dualmem \\\n",
    "    --seq_len 100 --delay 40 \\\n",
    "    --epochs 20 --batch_size 64 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OT Memory Agent\n",
    "!python -m transformer_killer_core.unified_bench \\\n",
    "    --mode synthetic --task copy_memory \\\n",
    "    --controller ot_agent \\\n",
    "    --seq_len 100 --delay 40 \\\n",
    "    --epochs 20 --batch_size 64 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Associative Recall Task\n",
    "\n",
    "Tests content-addressable memory retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all controllers on associative recall\n",
    "for controller in ['transformer', 'mamba', 'mamba_dualmem', 'ot_agent']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Controller: {controller}\")\n",
    "    print('='*60)\n",
    "    !python -m transformer_killer_core.unified_bench \\\n",
    "        --mode synthetic --task assoc_recall \\\n",
    "        --controller {controller} \\\n",
    "        --seq_len 30 --num_pairs 6 \\\n",
    "        --epochs 20 --batch_size 64 \\\n",
    "        --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Language Model Benchmark\n",
    "\n",
    "Character-level language modeling on a text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample corpus (Shakespeare)\n",
    "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O /content/corpus.txt\n",
    "!head -20 /content/corpus.txt\n",
    "!wc -c /content/corpus.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer LM\n",
    "!python -m transformer_killer_core.unified_bench \\\n",
    "    --mode lm \\\n",
    "    --controller transformer \\\n",
    "    --data_path /content/corpus.txt \\\n",
    "    --seq_len 256 --epochs 10 \\\n",
    "    --batch_size 32 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mamba + DualTierMiras LM\n",
    "!python -m transformer_killer_core.unified_bench \\\n",
    "    --mode lm \\\n",
    "    --controller mamba_dualmem \\\n",
    "    --data_path /content/corpus.txt \\\n",
    "    --seq_len 256 --epochs 10 \\\n",
    "    --batch_size 32 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OT Memory Agent LM\n",
    "!python -m transformer_killer_core.unified_bench \\\n",
    "    --mode lm \\\n",
    "    --controller ot_agent \\\n",
    "    --data_path /content/corpus.txt \\\n",
    "    --seq_len 256 --epochs 10 \\\n",
    "    --batch_size 32 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Comparison (All Controllers)\n",
    "\n",
    "Run a comprehensive comparison and save logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create logs directory\n",
    "!mkdir -p /content/logs\n",
    "\n",
    "controllers = ['transformer', 'mamba', 'mamba_dualmem', 'ot_agent']\n",
    "tasks = ['copy_memory', 'assoc_recall']\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for task in tasks:\n",
    "    for controller in controllers:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Task: {task} | Controller: {controller}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        extra_args = \"--delay 40\" if task == \"copy_memory\" else \"--num_pairs 6\"\n",
    "        seq_len = 100 if task == \"copy_memory\" else 30\n",
    "        \n",
    "        !python -m transformer_killer_core.unified_bench \\\n",
    "            --mode synthetic --task {task} \\\n",
    "            --controller {controller} \\\n",
    "            --seq_len {seq_len} {extra_args} \\\n",
    "            --epochs 20 --batch_size 64 \\\n",
    "            --device cuda \\\n",
    "            --log_dir /content/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View saved logs\n",
    "!ls -la /content/logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and display results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "log_dir = Path('/content/logs')\n",
    "results = []\n",
    "\n",
    "for log_file in sorted(log_dir.glob('*.jsonl')):\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            metadata = json.loads(lines[0]).get('metadata', {})\n",
    "            # Get final epoch result\n",
    "            if len(lines) > 1:\n",
    "                final = json.loads(lines[-1])\n",
    "                results.append({\n",
    "                    'task': metadata.get('task'),\n",
    "                    'controller': metadata.get('controller'),\n",
    "                    'final_val_acc': final.get('val_acc'),\n",
    "                    'final_loss': final.get('val_loss') or final.get('loss'),\n",
    "                })\n",
    "\n",
    "# Display as table\n",
    "print(f\"{'Task':<15} {'Controller':<15} {'Val Acc':<10} {'Loss':<10}\")\n",
    "print('-' * 50)\n",
    "for r in results:\n",
    "    acc = f\"{r['final_val_acc']:.4f}\" if r['final_val_acc'] else 'N/A'\n",
    "    loss = f\"{r['final_loss']:.4f}\" if r['final_loss'] else 'N/A'\n",
    "    print(f\"{r['task']:<15} {r['controller']:<15} {acc:<10} {loss:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download logs\n",
    "!zip -r /content/benchmark_logs.zip /content/logs\n",
    "\n",
    "from google.colab import files\n",
    "files.download('/content/benchmark_logs.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Experiments\n",
    "\n",
    "Modify parameters below for your own experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom experiment parameters\n",
    "CONTROLLER = \"mamba_dualmem\"  # transformer, mamba, mamba_dualmem, ot_agent\n",
    "TASK = \"copy_memory\"          # copy_memory, assoc_recall\n",
    "SEQ_LEN = 200                 # Sequence length\n",
    "DELAY = 80                    # Delay for copy_memory\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "D_MODEL = 128                 # Model dimension\n",
    "N_LAYERS = 3                  # Number of layers\n",
    "\n",
    "!python -m transformer_killer_core.unified_bench \\\n",
    "    --mode synthetic --task {TASK} \\\n",
    "    --controller {CONTROLLER} \\\n",
    "    --seq_len {SEQ_LEN} --delay {DELAY} \\\n",
    "    --epochs {EPOCHS} --batch_size {BATCH_SIZE} \\\n",
    "    --d_model {D_MODEL} --n_layers {N_LAYERS} \\\n",
    "    --device cuda \\\n",
    "    --log_dir /content/logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
